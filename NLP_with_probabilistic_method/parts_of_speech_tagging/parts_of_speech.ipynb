{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a11acf-6dd0-43df-9b98-71172173b52b",
   "metadata": {},
   "source": [
    "# In this note book I am implementing parts of speech tagging for a text_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33243c43-2715-41a7-ae10-8979eefdb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from utils_pos import get_word_tag, preprocess  \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f37e8-85fe-4ea5-bb20-3881c8197503",
   "metadata": {},
   "source": [
    "Here we are using two tagged data set that are collect from  **the Wall Street Journal (WSJ)**.\n",
    "\n",
    "1. One data set (WSJ-2_21.pos) will be used for training.\n",
    "2. The other (WSJ-24.pos) for testing.\r",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067d1108-918d-490e-97ad-d40f4d6366b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n', 'of\\tIN\\n', '``\\t``\\n', 'The\\tDT\\n', 'Misanthrope\\tNN\\n', \"''\\t''\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b59e32-cd65-444b-a6d2-c6e4b2e155e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "print(\"First few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fe5692-cd34-4193-9f6c-8e4c93589e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "':6\n",
      "'':7\n",
      "'40s:8\n",
      "'60s:9\n",
      "'70s:10\n",
      "'80s:11\n",
      "'86:12\n",
      "'90s:13\n",
      "'N:14\n",
      "'S:15\n",
      "'d:16\n",
      "'em:17\n",
      "'ll:18\n",
      "'m:19\n",
      "'n':20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {} \n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4c3ebd-d868-45a1-b365-e410dc274ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "\n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73192f5-4aa6-4d91-8786-8b60f4cb0613",
   "metadata": {},
   "source": [
    "The test set (WSJ-24.pos) is read in to create y.\n",
    "\n",
    "This contains both the test text and the true tag.\n",
    "    \n",
    "The test set has also been preprocessed to remove the tags to form **test_words.txt.**\n",
    "\n",
    "preprocess function take  word from test.words and check it is in vocab or not .If it is in vocab then we store it other wise instead of word we store unknown tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e5a43d-e1a8-4b89-967e-ab7c32902cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of  test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--', 'points', 'this', 'week', ',', 'with', 'readings', 'on', 'trade', ',', 'output']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of  test_corpus: ')\n",
    "print(prep[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "023bb992-97f6-4592-bfaf-471472f84e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dictionaries\n",
    "def create_dictionaries(training_corpus, vocab) :\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"word count = {i}\")\n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        word, tag = get_word_tag(word_tag,vocab) \n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6ca1ab-2011-46bd-8ec4-e87a5f9f9a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8f8c74-4c9e-4990-a810-43076b045475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989860"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7c27e9-b44c-4c74-b762-bbb5680237b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags  46\n",
      "Tags are followings :  ['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(\"Number of tags \", len(states))\n",
    "print(\"Tags are followings : \",states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb6fde9f-192c-421b-9da8-fa7f01c695b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of transition_counts\n",
      "\n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "Example of emission_count\n",
      "\n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "(('NNS', 'patterns'), 11)\n",
      "(('MD', 'might'), 334)\n",
      "\n",
      "Example of ambiguous \n",
      "\n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "#Show transition_counts, emission_count\n",
    "print('Example of transition_counts\\n')\n",
    "for count in list(transition_counts.items())[:3] :\n",
    "    print(count)\n",
    "print()\n",
    "print('Example of emission_count\\n')\n",
    "for em in list(emission_counts.items())[200:205] :\n",
    "    print(em)\n",
    "\n",
    "# A word which is called ambiguous if a word have many tag.\n",
    "print(\"\\nExample of ambiguous \\n\")\n",
    "for tup,count in emission_counts.items() :\n",
    "    if tup[1] == 'back' :\n",
    "        print(tup, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d49060-2420-44c3-8334-d605ef6eec73",
   "metadata": {},
   "source": [
    "# Assign a part of speech to a word and evaluate how well this approach work.\n",
    "To assign a part of speech to a word, assign the most frequent POS for that word in the training set.\n",
    "Then evaluate how well this approach works.\n",
    "Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!\n",
    "Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42b2bd8b-bf11-45d7-952d-5cc92071d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement predict_pos that computes the accuracy of your model\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    num_correct = 0  # initialize the number of correct prediction.\n",
    "    all_words = set(emission_counts.keys()) # all_words contains the all distinct (tag, words) in the training set.\n",
    "    total = len(y) # total is the number of distinct word in the test data set.\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        y_tup_l = y_tup.split() # Split the (word, POS) string into a list of two items.\n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                key = (pos,word)\n",
    "                if key in all_words:\n",
    "                    count = emission_counts[key]\n",
    "                    # For finding out maximum count of (tag, word) pair in the training data set\n",
    "                    if count>count_final:\n",
    "                        count_final = count\n",
    "                        pos_final = pos       \n",
    "        if pos_final == true_label:\n",
    "                num_correct += 1\n",
    "    accuracy = num_correct / total # accuracy calculation\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e95b7bc-aaec-41a9-871e-f5c57719d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8889\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, voc_l, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c985b5-bba1-4320-858d-184ef0d0e1fe",
   "metadata": {},
   "source": [
    "# Creatinig transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a2f44b4-5100-449a-a0cd-c23fd32bb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    all_tags = sorted(tag_counts.keys()) # contains all distinct tags\n",
    "    num_tags = len(all_tags) # number of distinct tags\n",
    "    transition_mat = np.zeros((num_tags, num_tags)) # initialize transition matrix\n",
    "    # Now creating transition matrix by using transition_counts dictionary\n",
    "    for i in range(num_tags) :\n",
    "        for j in range(num_tags) :\n",
    "            key = (all_tags[i], all_tags[j]) # form the key for transition_counts. \n",
    "            count=0\n",
    "            if key in transition_counts :\n",
    "                count = transition_counts[key] # store corresponding transition_count value.\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            transition_mat[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags) # Apply smoothing\n",
    "    return transition_mat       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9d67bdb-0693-4a86-98a0-ebdac2f54794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "#Example of transition matrix\n",
    "alpha = 0.001\n",
    "for i in range(46):\n",
    "    tag_counts.pop(i,None)\n",
    "\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "#print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c7c37-27c0-4581-a64d-b26962faeb29",
   "metadata": {},
   "source": [
    "# Creating emission matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd36810-790b-4b38-a084-407f3b5c1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    num_tags = len(tag_counts) # number of tag in vocab\n",
    "    all_tags = sorted(tag_counts.keys()) # store all distinct tag\n",
    "    num_words = len(vocab) # store number of distinct words in the vocab\n",
    "    emission_matrix = np.zeros((num_tags, num_words)) # initialize the emission_matrix\n",
    "    emis_keys = set(list(emission_counts.keys())) # store set of all (tag,word) pairs.\n",
    "    # Now creating emission matrix\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "            count = 0\n",
    "            key = (all_tags[i],vocab[j]) # form key for emission_counts\n",
    "            if key in emission_counts.keys(): # check if key is in emission_count dictionary key or not\n",
    "                count = emission_counts[key] # store corresponding key count\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            emission_matrix[i,j] = (count + alpha) / (count_tag+ alpha*num_words)# Doing smoothing\n",
    "    return emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dcf2e9cb-55df-465c-9da3-64ac14e98715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating emission probability matrix.\n",
    "for i in range(46):\n",
    "    tag_counts.pop(i,None)\n",
    "\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ddf1a-a2dd-46ff-8b2b-9b1897bc2675",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a2c16-f7cf-4a89-8dc4-98dc4b10ab68",
   "metadata": {},
   "source": [
    "# Initialize best_path and best_prob matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c3fec03-b07a-4149-bb6e-dda582945aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    num_tags = len(tag_counts) # store number of distinct tags\n",
    "    best_probs = np.zeros((num_tags, len(corpus))) # initialize the matrix best_probs of order (num_tags X (number of distinct word in test data set))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)# initialize the matrix best_paths of order (num_tags X (number of distinct word in test data set))\n",
    "    s_idx = states.index(\"--s--\") # store the index of the starting tag.\n",
    "    \n",
    "    # Now initialize first column of the best_probs\n",
    "    for i in range(num_tags):\n",
    "        if A[s_idx,i] == 0:\n",
    "            best_probs[i,0] = float('-inf')\n",
    "        else:\n",
    "            # vocab[corpus[0]] is the index of the first word of the corpus.\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fee8df36-c164-4de4-9456-43e92ff9750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing best_probs and best_paths\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1521ea8a-8443-4574-adf3-671155f39b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639ef27-6d6f-4d62-b55f-336d302a513c",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "853e2ae4-ce63-4cdf-b78d-0dbc6ab0428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    num_tags = best_probs.shape[0]\n",
    "    for i in range(1, len(test_corpus)):\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "        for j in range(num_tags):\n",
    "            #initialization the variable\n",
    "            best_prob_i =  float(\"-inf\")\n",
    "            best_path_i = None\n",
    "            # we store the index of the previous tag k in the best_path in which ith word have jth tag with high probability and \n",
    "            # store the corresponding highest probability in the (j,i) th cell of best_prob.\n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            best_paths[j,i] = best_path_i\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7dcac22-4166-408c-960d-2944e9d56c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50ce791e-3c83-413b-9bba-5c31a17e2260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db666a8-0de4-4189-b428-5893cd83aa1c",
   "metadata": {},
   "source": [
    "# Backward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf69e0ee-d96e-4144-b553-358c4465ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "        # Get the number of words in the corpus\n",
    "        # which is also the number of columns in best_probs, best_paths\n",
    "        m = best_paths.shape[1]\n",
    "        z = [None] * m # Initialize array z, same length as the corpus\n",
    "        num_tags = best_probs.shape[0]\n",
    "        best_prob_for_last_word = float('-inf')\n",
    "        pred = [None] * m\n",
    "        for k in range(num_tags):\n",
    "            if best_probs[k,-1]>best_prob_for_last_word:\n",
    "                best_prob_for_last_word = best_probs[k,-1]\n",
    "                z[m - 1]=k\n",
    "        pred[m - 1] = states[k]\n",
    "        for i in range(len(corpus)-2, -1, -1):\n",
    "            z[i] = best_paths[z[i+1], i+1]\n",
    "            pred[i] = states[z[i]]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "805fea0c-b037-4e9c-be3b-06167be06a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3432d77-d182-48d6-941f-a9e5b6a0c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(pred, y):\n",
    "    num_correct = 0 #initialize the number of correct prediction\n",
    "    total = 0 \n",
    "    for prediction, y in zip(pred, y):\n",
    "        word_tag_tuple = y.split() # split in the form of list (word, tag)\n",
    "        if len(word_tag_tuple)!=2:\n",
    "            continue \n",
    "        word, tag = word_tag_tuple\n",
    "        if prediction == tag: # if prediction id is correct increase the number of correct prediction by 1.\n",
    "            num_correct += 1\n",
    "        total += 1 # at the end of the loop total contains the number of total word in y.\n",
    "    return (num_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "388343fc-d4e3-4669-9093-3b66005cc86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2da69cd4-1770-4663-9774-087d8813cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mrinmoy\n",
      "[nltk_data]     Bera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a53eb-3bec-4cc9-ac14-3f80380f8ff9",
   "metadata": {},
   "source": [
    "# parts_of_speech_tagging of a text file \n",
    "Input a text file following function return corresponding parts of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbb735c5-0d2d-4fff-b657-026a75e9af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech_tagging(text_file, states, tag_counts, A, B, vocab) :\n",
    "    _, prep = preprocess(vocab, text_file)  \n",
    "    \n",
    "    # Initializing best_probs and best_paths\n",
    "    best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "\n",
    "    #veterbi_forward step\n",
    "    best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "\n",
    "    #veterbi_backward\n",
    "    pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e6433a3-b88c-47f3-8fbc-e6422118841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "#prediction of corresponding text file\n",
    "pred = parts_of_speech_tagging(\"test.words\", states, tag_counts, A, B, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd3848b7-a218-4a48-a6a9-ef70e7610b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Eample of parts_of_speech_tagging \n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
